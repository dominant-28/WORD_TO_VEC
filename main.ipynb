{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords and punkt for tokenization\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample paragraph\n",
    "paragraph = \"\"\"Word2Vec is a popular algorithm used to transform words into continuous vector representations, capturing their semantic and syntactic relationships. \n",
    "It uses two key architectures: Continuous Bag of Words (CBOW), which predicts a target word from its surrounding context, and Skip-gram, which predicts the context words given a target word. \n",
    "By training a neural network on a large corpus of text, Word2Vec learns dense embeddings where similar words are closer in vector space. \n",
    "These embeddings can be used in downstream tasks such as text classification, clustering, or as input features for machine learning models. \n",
    "Implementation typically involves libraries like Gensim or TensorFlow, where preprocessing includes tokenization, removal of stopwords, and creating a vocabulary from the dataset. \n",
    "After training, the model generates vector representations that can be queried for similarity or arithmetic operations to explore relationships between words.T\"\"\"\n",
    "\n",
    "# Preprocessing the paragraph (lowercase, remove punctuation, and tokenize)\n",
    "text = paragraph.lower()  # Convert text to lowercase\n",
    "text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "tokens = nltk.word_tokenize(text)  # Tokenize the text\n",
    "\n",
    "# Optional: Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Prepare context-target pairs with a window size of 2\n",
    "data = []\n",
    "\n",
    "for i in range(2, len(tokens) - 2):\n",
    "    context = [tokens[i-2], tokens[i-1], tokens[i+1], tokens[i+2]]\n",
    "    target = tokens[i]\n",
    "    data.append((context, target))\n",
    "\n",
    "# Display the first 5 context-target pairs\n",
    "print(f\"Sample Context-Target Pairs: {data[:5]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab= tokens\n",
    "word_to_idx ={word: idx for idx,word in enumerate(vocab)}\n",
    "id_to_word= {idx:word for word,idx in word_to_idx.items()}\n",
    "\n",
    "encoded_data=[([word_to_idx[word] for word in context], word_to_idx[target]) for context,target in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wordtovecdataset(Dataset):\n",
    "    def __init__(self,data):\n",
    "        self.data=data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        context ,target=self.data[idx]   \n",
    "        return torch.tensor(context,dtype=torch.long),torch.tensor(target,dtype=torch.long) \n",
    "dataset = wordtovecdataset(encoded_data)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim):\n",
    "        super(word2vec,self).__init__()\n",
    "        self.embeddings=nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.linear=nn.Linear(embedding_dim,vocab_size)\n",
    "\n",
    "    def forward(self,context):\n",
    "        contexemb=self.embeddings(context)\n",
    "        contextemb=contexemb.mean(dim=1) \n",
    "        output=self.linear(contextemb)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=100\n",
    "vocab_size=len(vocab)\n",
    "\n",
    "model=word2vec(vocab_size,embedding_dim)\n",
    "\n",
    "\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs=300\n",
    "for epoch in range(epochs):\n",
    "    total_loss=0\n",
    "    for context,target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output=model(context)\n",
    "        \n",
    "        loss= criterion(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Extract all embeddings (weights from the embedding layer)\n",
    "word_embeddings = model.embeddings.weight.data\n",
    "\n",
    "# Reduce to 3D using PCA\n",
    "pca = PCA(n_components=3)\n",
    "reduced_embeddings = pca.fit_transform(word_embeddings.numpy())  # Convert to NumPy for PCA\n",
    "\n",
    "# Example: Visualize a single word in 3D\n",
    "\n",
    "\n",
    "# Visualize all embeddings for context\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each word's 3D embedding\n",
    "count=0\n",
    "for word, idx in word_to_idx.items():\n",
    "    count=count+1\n",
    "    ax.scatter(reduced_embeddings[idx, 0], reduced_embeddings[idx, 1], reduced_embeddings[idx, 2], label=word)\n",
    "    ax.text(reduced_embeddings[idx, 0], reduced_embeddings[idx, 1], reduced_embeddings[idx, 2], word, fontsize=8)\n",
    "    if(count>50):\n",
    "        break\n",
    "\n",
    "ax.set_title(\"3D Visualization of Word Embeddings\")\n",
    "ax.set_xlabel(\"PCA Component 1\")\n",
    "ax.set_ylabel(\"PCA Component 2\")\n",
    "ax.set_zlabel(\"PCA Component 3\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Extract the embedding matrix from the model\n",
    "embedding_matrix = model.embeddings.weight.data  # Shape: (vocab_size, embedding_dim)\n",
    "\n",
    "# Function to find the most similar word\n",
    "def find_most_similar(word, word_to_idx, id_to_word, embedding_matrix, top_n=5):\n",
    "    # Get the index of the input word\n",
    "    word_idx = word_to_idx[word]\n",
    "    \n",
    "    # Get the embedding of the input word\n",
    "    word_embedding = embedding_matrix[word_idx]  # Shape: (embedding_dim,)\n",
    "    \n",
    "    # Compute cosine similarity between the word embedding and all other embeddings\n",
    "    similarities = F.cosine_similarity(word_embedding.unsqueeze(0), embedding_matrix, dim=1)\n",
    "    \n",
    "    # Get the top N most similar words (excluding the input word itself)\n",
    "    top_n_indices = torch.topk(similarities, top_n + 1).indices  # +1 to exclude the word itself\n",
    "    top_n_indices = top_n_indices[top_n_indices != word_idx][:top_n]\n",
    "    \n",
    "    # Convert indices back to words\n",
    "    similar_words = [id_to_word[idx.item()] for idx in top_n_indices]\n",
    "    \n",
    "    return similar_words\n",
    "\n",
    "# Test the function\n",
    "word = \"architectures\"\n",
    "most_similar_words = find_most_similar(word, word_to_idx, id_to_word, embedding_matrix, top_n=5)\n",
    "print(f\"Words most similar to '{word}': {most_similar_words}\")\n",
    "\n",
    "word = \"implementation\"\n",
    "most_similar_words = find_most_similar(word, word_to_idx, id_to_word, embedding_matrix, top_n=5)\n",
    "print(f\"Words most similar to '{word}': {most_similar_words}\")\n",
    "\n",
    "word = \"algorithm\"\n",
    "most_similar_words = find_most_similar(word, word_to_idx, id_to_word, embedding_matrix, top_n=5)\n",
    "print(f\"Words most similar to '{word}': {most_similar_words}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
